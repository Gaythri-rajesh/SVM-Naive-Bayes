{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuxdWZAAZrjR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM & Naive Bayes\n",
        "\n",
        "1. What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "-> 1. What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. Its main goal is to find the optimal decision boundary (hyperplane) that best separates the data points of different classes in a feature space.\n",
        "\n",
        "How it works:\n",
        "\n",
        "Finding the Hyperplane:\n",
        "\n",
        "In a two-dimensional space, the hyperplane is a line that separates the data points of different classes.\n",
        "\n",
        "In higher dimensions, it becomes a plane or multidimensional surface.\n",
        "\n",
        "SVM tries to find the hyperplane that maximizes the margin, i.e., the distance between the hyperplane and the nearest data points from each class.\n",
        "\n",
        "Support Vectors:\n",
        "\n",
        "The data points that are closest to the hyperplane and influence its position are called support vectors.\n",
        "\n",
        "These points are critical, as removing them would change the decision boundary.\n",
        "\n",
        "Maximizing the Margin:\n",
        "\n",
        "The optimal hyperplane is the one that gives the largest margin (the widest gap between classes).\n",
        "\n",
        "Mathematically, this becomes a quadratic optimization problem that minimizes the classification error while maximizing the margin.\n",
        "\n",
        "Non-linear Separation (Kernel Trick):\n",
        "\n",
        "If data is not linearly separable, SVM uses a kernel function (e.g., polynomial, RBF) to transform data into a higher-dimensional space where it can be linearly separated.\n",
        "\n",
        "2. Explain the difference between Hard Margin and Soft Margin SVM\n",
        "\n",
        "-> 1. Hard Margin SVM:\n",
        "In a hard margin SVM, the algorithm assumes that the data is perfectly linearly separable. It tries to find a hyperplane that separates the two classes without allowing any misclassification or overlap. Every data point must be correctly classified and must lie outside the margin boundaries.\n",
        "This approach works well only when the data is clean and there is a clear gap between classes. However, it‚Äôs very sensitive to noise and outliers, because even one misclassified or noisy point can make it impossible to find a valid hyperplane.\n",
        "\n",
        "2.Soft Margin SVM:\n",
        "The soft margin approach is more practical for real-world data, which often has noise or overlapping points. It allows some points to violate the margin or be misclassified, introducing slack variables to measure how much each point deviates from the ideal margin.\n",
        "A penalty parameter (C) controls this trade-off:\n",
        "\n",
        "A large C tries to minimize misclassification (behaves more like a hard margin).\n",
        "\n",
        "A small C allows more violations to get a smoother, more generalizable boundary.\n",
        "\n",
        "In simple terms, soft margin SVM balances between maximizing the margin and allowing small classification errors to avoid overfitting and handle complex data more effectively.\n",
        "\n",
        "3. What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "\n",
        "-> The Kernel Trick is a mathematical technique used in Support Vector Machines (SVM) to handle non-linearly separable data.\n",
        "Instead of transforming the data into a higher-dimensional space manually, the kernel trick allows SVM to implicitly map the data into a higher-dimensional feature space, where a linear separation becomes possible ‚Äî all without explicitly computing the coordinates in that space.\n",
        "\n",
        "This saves a lot of computation and makes it possible to separate data that cannot be divided by a straight line in its original form.\n",
        "\n",
        "Example ‚Äì Radial Basis Function (RBF) Kernel:\n",
        "The RBF (Gaussian) kernel is one of the most commonly used kernels. It measures similarity between two data points using the distance between them. The formula is:\n",
        "\n",
        "ùêæ\n",
        "(\n",
        "ùë•\n",
        ",\n",
        "ùë•\n",
        "‚Ä≤\n",
        ")\n",
        "=\n",
        "exp\n",
        "‚Å°\n",
        "(\n",
        "‚àí\n",
        "ùõæ\n",
        "‚à£\n",
        "‚à£\n",
        "ùë•\n",
        "‚àí\n",
        "ùë•\n",
        "‚Ä≤\n",
        "‚à£\n",
        "‚à£\n",
        "2\n",
        ")\n",
        "K(x,x\n",
        "‚Ä≤\n",
        ")=exp(‚àíŒ≥‚à£‚à£x‚àíx\n",
        "‚Ä≤\n",
        "‚à£‚à£\n",
        "2\n",
        ")\n",
        "\n",
        "Here, Œ≥ (gamma) determines how far the influence of a single training point reaches.\n",
        "\n",
        "Use case:\n",
        "RBF kernels are widely used when data is non-linear ‚Äî for example, in classifying medical records, image recognition, or financial fraud detection ‚Äî where the relationship between features and classes is complex and cannot be captured by a simple straight line.\n",
        "\n",
        "4. What is a Na√Øve Bayes Classifier, and why is it called ‚Äúna√Øve‚Äù?\n",
        "\n",
        "-> A Na√Øve Bayes Classifier is a probabilistic machine learning model based on Bayes‚Äô Theorem, used primarily for classification tasks.\n",
        "It predicts the class of a data point by calculating the posterior probability of each class given the input features and selecting the class with the highest probability.\n",
        "\n",
        "Bayes‚Äô Theorem is given as:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        "‚à£\n",
        "ùëã\n",
        ")\n",
        "=\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        "‚à£\n",
        "ùê∂\n",
        ")\n",
        "‚ãÖ\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        ")\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        ")\n",
        "P(C‚à£X)=\n",
        "P(X)\n",
        "P(X‚à£C)‚ãÖP(C)\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        "‚à£\n",
        "ùëã\n",
        ")\n",
        "P(C‚à£X): Posterior probability of class\n",
        "ùê∂\n",
        "C given features\n",
        "ùëã\n",
        "X\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        "‚à£\n",
        "ùê∂\n",
        ")\n",
        "P(X‚à£C): Likelihood of features given class\n",
        "ùê∂\n",
        "C\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        ")\n",
        "P(C): Prior probability of class\n",
        "ùê∂\n",
        "C\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        ")\n",
        "P(X): Probability of the features\n",
        "\n",
        "It is called ‚Äúna√Øve‚Äù because it assumes that all features are independent of each other given the class label.\n",
        "In real-world data, this independence assumption is rarely true ‚Äî hence the name ‚Äúna√Øve‚Äù ‚Äî but despite this simplification, the model performs remarkably well in many applications such as spam detection, sentiment analysis, and document classification.\n",
        "\n",
        "5. Describe the Gaussian, Multinomial, and Bernoulli Na√Øve Bayes variants. When would you use each one?\n",
        "\n",
        "-> Gaussian Na√Øve Bayes:\n",
        "\n",
        "Assumes that the features follow a normal (Gaussian) distribution.\n",
        "\n",
        "Suitable for continuous data, such as height, weight, temperature, or sensor readings.\n",
        "\n",
        "Use case: Predicting whether a patient has a disease based on continuous medical test results.\n",
        "\n",
        "Multinomial Na√Øve Bayes:\n",
        "\n",
        "Used when features represent counts or frequencies (non-negative integers).\n",
        "\n",
        "Commonly applied in text classification, where features are word counts or term frequencies.\n",
        "\n",
        "Use case: Classifying emails as spam or not spam using word occurrence counts.\n",
        "\n",
        "Bernoulli Na√Øve Bayes:\n",
        "\n",
        "Used for binary/boolean features, where each feature can take only two values (e.g., 0 or 1).\n",
        "\n",
        "It models the presence or absence of a particular feature.\n",
        "\n",
        "Use case: Document classification where we only care whether a word appears in a document (not how many times).\n",
        "\n",
        "6. Write a Python program to: Load the Iris dataset Train an SVM Classifier with a linear kernelPrint the model‚Äôs accuracy and support vectors\n"
      ],
      "metadata": {
        "id": "0BxVlDtsZ1P3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier with linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Print model accuracy and support vectors\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Number of Support Vectors for each class:\", svm_model.n_support_)\n",
        "print(\"Support Vectors:\\n\", svm_model.support_vectors_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAT7iZ64b_vA",
        "outputId": "9d1db222-6c85-4148-c8fb-60e9ab3dbd20"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Number of Support Vectors for each class: [ 3 11 10]\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "‚óè Load the Breast Cancer dataset\n",
        "‚óè Train a Gaussian Na√Øve Bayes model\n",
        "‚óè Print its classification report including precision, recall, and F1-score.\n"
      ],
      "metadata": {
        "id": "-IhsvrmdcKBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naive Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UQqTFWbcPZv",
        "outputId": "ec431e7e-d1f6-4b68-db92-e631de3cef97"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.90      0.92        63\n",
            "           1       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to: ‚óè Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma. ‚óè Print the best hyperparameters and accuracy."
      ],
      "metadata": {
        "id": "BVn5EVmVcUi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define parameter grid for C and gamma\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=0)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = grid.predict(X_test)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Hyperparameters:\", grid.best_params_)\n",
        "print(\"Best Model Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDKBJJ2xcjPr",
        "outputId": "2e5f8a5d-5678-4857-90ae-d7e3ee84a061"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Best Model Accuracy: 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "‚óè Train a Na√Øve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "‚óè Print the model's ROC-AUC score for its predictions.\n"
      ],
      "metadata": {
        "id": "lRMJHomEcna0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load text dataset\n",
        "data = fetch_20newsgroups(subset='all', categories=['rec.sport.baseball', 'sci.space', 'comp.graphics'], remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Convert text data into TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Multinomial Na√Øve Bayes classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict probabilities for ROC-AUC\n",
        "y_pred_prob = nb_model.predict_proba(X_test_tfidf)\n",
        "\n",
        "# Binarize labels (for multi-class ROC-AUC)\n",
        "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
        "\n",
        "# Calculate ROC-AUC score (macro average)\n",
        "roc_auc = roc_auc_score(y_test_bin, y_pred_prob, average='macro')\n",
        "\n",
        "# Print the ROC-AUC score\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpUUjL3RctXB",
        "outputId": "20d358e2-2847-4f77-cf20-c86b1ce319e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9895453018243434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you‚Äôre working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "‚óè Text with diverse vocabulary\n",
        "‚óè Potential class imbalance (far more legitimate emails than spam)\n",
        "‚óè Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "‚óè Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "‚óè Choose and justify an appropriate model (SVM vs. Na√Øve Bayes)\n",
        "‚óè Address class imbalance\n",
        "‚óè Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n"
      ],
      "metadata": {
        "id": "rqLrfO6dc2b3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Load dataset (using categories similar to spam vs. not spam)\n",
        "categories = ['sci.electronics', 'talk.politics.misc']  # assume 'sci.electronics' as 'not spam' and 'talk.politics.misc' as 'spam'\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Create DataFrame for easier handling\n",
        "df = pd.DataFrame({'text': data.data, 'target': data.target})\n",
        "\n",
        "# Handle missing data (replace missing text with 'unknown')\n",
        "df['text'].fillna('unknown', inplace=True)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], test_size=0.3, random_state=42, stratify=df['target'])\n",
        "\n",
        "# Convert text data into numerical form using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Handle class imbalance using oversampling\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_res, y_res = oversampler.fit_resample(X_train_tfidf, y_train)\n",
        "\n",
        "# Train Naive Bayes classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_res, y_res)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = nb_model.predict(X_test_tfidf)\n",
        "y_prob = nb_model.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\n=== Confusion Matrix ===\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\n=== ROC-AUC Score ===\")\n",
        "print(roc_auc_score(y_test, y_prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oemh8xJXdLl1",
        "outputId": "1a61e1ec-8dde-4ca2-ea4e-7e288b9d3e24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3076778801.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['text'].fillna('unknown', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.94      0.94       295\n",
            "           1       0.93      0.93      0.93       233\n",
            "\n",
            "    accuracy                           0.94       528\n",
            "   macro avg       0.93      0.93      0.93       528\n",
            "weighted avg       0.94      0.94      0.94       528\n",
            "\n",
            "\n",
            "=== Confusion Matrix ===\n",
            "[[278  17]\n",
            " [ 17 216]]\n",
            "\n",
            "=== ROC-AUC Score ===\n",
            "0.9891612715501564\n"
          ]
        }
      ]
    }
  ]
}